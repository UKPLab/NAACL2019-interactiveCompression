# Data-efficient Neural Text Compression with Interactive Learning

In this project, we develop a general framework for Interactive Text Compression. We propose an interactive text compression model using active learning learning methods for data-efficient learning.

If you reuse this software, please use the following citation:

```
@inproceedings{tubiblio111696,
    title = {Data-efficient Neural Text Compression with Interactive Learning},
    author = {P.V.S., Avinesh and Meyer, Christian M.},
    publisher = {Association for Computational Linguistics},
    booktitle = {Proceedings of the 17th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    pages = {(to appear)},
    month = june,
    year = {2019},
    location = {Minneapolis, USA},
}
```
> **Abstract:** Neural sequence-to-sequence models have been successfully applied to text compression. However, these models were trained on huge automatically induced parallel corpora, which are only available for a few domains and tasks. In this paper, we propose a novel interactive setup to neural text compression that enables transferring a model to new domains and compression tasks with minimal human supervision. This is achieved by employing active learning, which intelligently samples from a large pool of unlabeled data. Using this setup, we can successfully adapt a model trained on small data of 40k samples for a headline generation task to a general text compression dataset at an acceptable compression quality with just 500 sampled instances annotated by a human. 

Contact person: Avinesh P.V.S., avinesh@aiphes.tu-darmstadt.de

http://www.ukp.tu-darmstadt.de/

http://www.tu-darmstadt.de/

Don't hesitate to send us an e-mail or report an issue, if something is broken (and it shouldn't be) or if you have further questions.

> This repository contains experimental software and is published for the sole purpose of giving additional background details on the respective publication. 

